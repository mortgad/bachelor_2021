380/1.27
299.2126 +8
11867*10
118670*100
seq(1,298,1)
sample(seq(1,298,1),15)
3.50/11
3.44/11.2
library(tm)#
#
library(ggplot2)#
#
library(lsa)#
#
library(scatterplot3d)#
#
library(SnowballC)#
#------------------------------------------------------------------------------#
# 1. Prepare data from http://goo.gl/1RB32f#
text <- c("The discipline of phenomenology is defined by its domain of study, #
#
  its methods, and its main results.",#
"Phenomenology studies structures of conscious experience as experienced from #
#
the first-person point of view, along with relevant conditions of experience. #
#
The central structure of an experience is its intentionality, the way it is #
#
directed through its content or meaning toward a certain object in the world.",#
"We all experience various types of experience including perception, #
#
imagination, thought, emotion, desire, volition, and action. Thus, the domain #
#
of phenomenology is the range of experiences including these types (among #
#
  others). Experience includes not only relatively passive experience as in #
#
vision or hearing, but also active experience as in walking or hammering a #
#
nail or kicking a ball. (The range will be specific to each species of being #
#
  that enjoys consciousness; our focus is on our own, human, experience. Not #
#
  all conscious beings will, or will be able to, practice phenomenology, as #
#
  we do.)",#
"Conscious experiences have a unique feature: we experience them, we live #
#
through them or perform them. Other things in the world we may observe and #
#
engage. But we do not experience them, in the sense of living through or #
#
performing them. This experiential or first-person feature — that of being #
#
experienced — is an essential part of the nature or structure of conscious #
#
experience: as we say, “I see / think / desire / do …” This feature is both #
#
a phenomenological and an ontological feature of each experience: it is part #
#
of what it is for the experience to be experienced (phenomenological) and part #
#
of what it is for the experience to be (ontological).",#
"How shall we study conscious experience? We reflect on various types of #
#
experiences just as we experience them. That is to say, we proceed from the#
#
 first-person point of view. However, we do not normally characterize an #
#
 experience at the time we are performing it. In many cases we do not have#
#
  that capability: a state of intense anger or fear, for example, consumes #
#
  all of one's psychic focus at the time. Rather, we acquire a background #
#
  of having lived through a given type of experience, and we look to our #
#
  familiarity with that type of experience: hearing a song, seeing a sunset, #
#
  thinking about love, intending to jump a hurdle. The practice of #
#
  phenomenology assumes such familiarity with the type of experiences to #
#
  be characterized. Importantly, also, it is types of experience that #
#
  phenomenology pursues, rather than a particular fleeting experience #
#
  — unless its type is what interests us.",#
"Classical phenomenologists practiced some three distinguishable methods. #
#
(1) We describe a type of experience just as we find it in our own (past) #
#
experience. Thus, Husserl and Merleau-Ponty spoke of pure description of #
#
lived experience. (2) We interpret a type of experience by relating it to #
#
relevant features of context. In this vein, Heidegger and his followers #
#
spoke of hermeneutics, the art of interpretation in context, especially #
#
social and linguistic context. (3) We analyze the form of a type of experience. #
#
In the end, all the classical phenomenologists practiced analysis of #
#
experience, factoring out notable features for further elaboration.",#
"These traditional methods have been ramified in recent decades, expanding #
#
the methods available to phenomenology. Thus: (4) In a logico-semantic model #
#
of phenomenology, we specify the truth conditions for a type of thinking #
#
(say, where I think that dogs chase cats) or the satisfaction conditions for #
#
a type of intention (say, where I intend or will to jump that hurdle). (5) #
#
In the experimental paradigm of cognitive neuroscience, we design empirical #
#
experiments that tend to confirm or refute aspects of experience (say, #
#
  where a brain scan shows electrochemical activity in a specific region #
#
  of the brain thought to subserve a type of vision or emotion or motor #
#
  control). This style of “neurophenomenology” assumes that conscious #
#
experience is grounded in neural activity in embodied action in appropriate #
#
surroundings — mixing pure phenomenology with biological and physical #
#
science in a way that was not wholly congenial to traditional #
#
phenomenologists.",#
"What makes an experience conscious is a certain awareness one has of the experience while living through or performing it. This form of inner awareness has been a topic of considerable debate, centuries after the issue arose with Locke's notion of self-consciousness on the heels of Descartes' sense of consciousness (conscience, co-knowledge). Does this awareness-of-experience consist in a kind of inner observation of the experience, as if one were doing two things at once? (Brentano argued no.) Is it a higher-order perception of one's mind's operation, or is it a higher-order thought about one's mental activity? (Recent theorists have proposed both.) Or is it a different form of inherent structure? (Sartre took this line, drawing on Brentano and Husserl.) These issues are beyond the scope of this article, but notice that these results of phenomenological analysis shape the characterization of the domain of study and the methodology appropriate to the domain. For awareness-of-experience is a defining trait
of conscious experience, the trait that gives experience a first-person, lived character. It is that lived character of experience that allows a first-person perspective on the object of study, namely, experience, and that perspective is characteristic of the methodology of phenomenology.",#
"Conscious experience is the starting point of phenomenology, but experience shades off into less overtly conscious phenomena. As Husserl and others stressed, we are only vaguely aware of things in the margin or periphery of attention, and we are only implicitly aware of the wider horizon of things in the world around us. Moreover, as Heidegger stressed, in practical activities like walking along, or hammering a nail, or speaking our native tongue, we are not explicitly conscious of our habitual patterns of action. Furthermore, as psychoanalysts have stressed, much of our intentional mental activity is not conscious at all, but may become conscious in the process of therapy or interrogation, as we come to realize how we feel or think about something. We should allow, then, that the domain of phenomenology — our own experience — spreads out from conscious experience into semi-conscious and even unconscious mental activity, along with relevant background conditions implicitly invoked in our experience. (T
hese issues are subject to debate; the point here is to open the door to the question of where to draw the boundary of the domain of phenomenology.)")#
view <- factor(rep(c("view 1", "view 2", "view 3"), each = 3))#
#
view#
#
df <- data.frame(text, view, stringsAsFactors = FALSE)#
#
df
# prepare corpus#
#
corpus <- Corpus(VectorSource(df$text))#
#
corpus <- tm_map(corpus, tolower)#
#
corpus <- tm_map(corpus, removePunctuation)#
#
corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))#
# error below, can't stem#
#
corpus  <- tm_map(corpus, stemDocument, language = "english") #
#
corpus  #
#------------------------------------------------------------------------------#
# MDS with raw term-document matrix compute distance matrix#
#
td.mat <- as.matrix(TermDocumentMatrix(corpus))#
#
td.mat#
#
dist.mat <- dist(t(as.matrix(td.mat)))#
#
dist.mat  # check distance matrix#
#------------------------------------------------------------------------------#
# MDS#
#
fit <- cmdscale(dist.mat, eig = TRUE, k = 2)#
#
points <- data.frame(x = fit$points[, 1], y = fit$points[, 2])#
#
ggplot(points, aes(x = x, y = y)) + geom_point(data = points, aes(x = x, y = y, #
#
    color = df$view)) + geom_text(data = points, aes(x = x, y = y - 0.2, label = #
#
    row.names(df)))#
#------------------------------------------------------------------------------#
# MDS with LSA#
#
td.mat.lsa <- lw_bintf(td.mat) * gw_idf(td.mat)  # weighting#
#
lsaSpace <- lsa(td.mat.lsa)  # create LSA space#
#
dist.mat.lsa <- dist(t(as.textmatrix(lsaSpace)))  # compute distance matrix#
#
dist.mat.lsa  # check distance mantrix
corpus
td.mat <- as.matrix(TermDocumentMatrix(corpus))
corpus <- Corpus(VectorSource(df$text))#
#
corpus <- tm_map(corpus, tolower)#
#
corpus <- tm_map(corpus, removePunctuation)#
#
corpus <- tm_map(corpus, function(x) removeWords(x, stopwords("english")))#
# error below, can't stem#
#
corpus  <- tm_map(corpus, stemDocument, language = "english") #
#
corpus
df$text
# MDS with raw term-document matrix compute distance matrix#
#
td.mat <- as.matrix(TermDocumentMatrix(corpus))#
#
td.mat#
#
dist.mat <- dist(t(as.matrix(td.mat)))#
#
dist.mat  # check distance matrix
TermDocumentMatrix(corpus)
corpus
dist.mat <- dist(t(as.matrix(td.mat)))
librarypath()
library()
library.path()
.libPaths()
library(data.table)
license()
R --vanilla
getwd()
rm(list = ls())
update.packages()
y#
y
0.3375*2
08*1.2
0.8*1.2
=0.675*.8
0.675*.8
x <- sample(seq(from = 20, to = 50, by = 5), size = 50, replace = TRUE)
x
y <- sample(seq(from = 20, to = 200, by = 5), size = 100, replace = TRUE)
t.test(x,y)
t.test(x,y,alternative="greater")
t.test(x,y,alternative="lesser")
t.test(x,y,alternative="less")
78.50000+ 77.50000
8.40187000	+7.86600000	+8.00000000
24.26787-5
19.26787-5
14.26787-5
9.26787-5
library(data.table)
169.50000+84.99998
169.50000+58
169.50000+85
length(seq(1797,2015,))
length(seq(1797,2015,1))
?search
search()
data<-read.csv("http://andy.egge.rs/data/L.csv")#
# Test#
View(data)#
#
# Scatter diagram: Government Effectiveness and Executive-Parties dimension#
plot(data$exec_parties_1945_2010, data$govt_effectiveness_1996_2009, main#
 = "Figure 1", xlab = "Executive-Parties Dimension", ylab = "Government Effectiveness (1996-2009)")#
# Add in country name labels#
text(data$exec_parties_1945_2010, data$govt_effectiveness_1996_2009, labels#
 = data$country, cex = 0.5)#
# Perform regression#
model1 <- lm(data$govt_effectiveness_1996_2009 ~ data$exec_parties_1945_2010)#
summary(model1)#
# Add in regression line for Figure 1#
abline(model1)#
# Correlation#
cor.test(data$exec_parties_1945_2010,data$govt_effectiveness_1996_2009)#
# t test#
t.test(data$exec_parties_1945_2010,data$govt_effectiveness_1996_2009)#
#
# As above but controlling for population and level of development (HDI)#
plot(data$exec_parties_1945_2010, data$govt_effectiveness_1996_2009, main#
 = "Figure 2", xlab = "Executive-Parties Dimension", ylab = "Government Effectiveness (1996-2009)")#
# Add in country name labels#
text(data$exec_parties_1945_2010, data$govt_effectiveness_1996_2009, labels#
 = data$country, cex = 0.5)#
# Perform multivariate regression (part 1)#
model2 <- lm(data$govt_effectiveness_1996_2009 ~ data$exec_parties_1945_2010#
 + data$pop_in_thousands_2009)#
summary(model2)#
# Perform multivariate regression (part 2)#
model3 <- lm(data$govt_effectiveness_1996_2009 ~ data$exec_parties_1945_2010#
 + data$pop_in_thousands_2009 + data$hdi_2010)#
summary(model3)#
# Add in regression line for Figure 2#
abline(model3)#
# Correlation#
cor.test(data$exec_parties_1945_2010,data$govt_effectiveness_1996_2009)#
# t test#
t.test(data$exec_parties_1945_2010,data$govt_effectiveness_1996_2009)
abline(model3)
plot(data$exec_parties_1945_2010, data$govt_effectiveness_1996_2009, main
= "Figure 2", xlab = "Executive-Parties Dimension", ylab = "Government Effectiveness (1996-2009)")
abline(model3)
?abline
abline(coef(model3))
abline(coef-model3))
abline(coef=model3))
abline(coef=model3)
abline(a=coef(model3))
termplot(model3, terms="exec_parties_1945_2010")
termplot(model3, terms="govt_effectiveness_1996_2009")
termplot(model3)
ggplot(data, aes(x = govt_effectiveness_1996_2009, y = exec_parties_1945_2010)) + #
  geom_point() +#
  stat_smooth(method = "lm", col = "red")
library(ggplot2)
ggplot(data, aes(x = govt_effectiveness_1996_2009, y = exec_parties_1945_2010)) + #
  geom_point() +#
  stat_smooth(method = "lm", col = "red")
1777/5
1797/5
2413/5
20*14
15+13
15*13
6*67
402 + 195 + 280
set.seed(12345)#
setwd("/nfs/projects_nobackup/n/ngoet/dphil_machine_learning/matrices_no_procedural_strip")#
library(base)#
library(ca)#
library(Hmisc)#
library(data.table)#
#
#create incrementer#
inc <- function(x)#
{#
  eval.parent(substitute(x <- x + 1))#
}#
#
#load dataset#
system.time(dataset <- fread("uk_speeches_cleaned_new.csv",header=T,stringsAsFactors = FALSE))#
#exclude marginal parties#
# dataset_pre1922 <- dataset[dataset$party %in% c("C","LIB") & dataset$session_indicator < 132,]#
# dataset_post1922 <- dataset[dataset$party %in% c("C","LAB") & dataset$session_indicator >= 132,]#
#
# dataset <- rbind(dataset_pre1922, dataset_post1922)#
#
custom_list <- c("a","an","and","are","as","at","be","by","for","from",#
        "has","he","in","is","it","its","of","on","that","the",#
        "to","was","were","will","with")#
#
library(tm)#
library(austin)#
library(methods)#
library(data.table())#
#divide dataset by session#
data_per_session <- split(dataset,dataset$session_ref)#
rm(list=c("dataset"))#
#loop over #
for(s in 1:length(data_per_session)){#
#
  #store data for this loop#
  session_data <- data_per_session[[s]]#
  #aggregate all speeches by member and party#
  session_data <- aggregate(session_data$speech_content,by=list(session_data$matched_name,session_data$role,session_data$party,session_data$session_ref,session_data$session_indicator),paste,collapse=",")#
  colnames(session_data) <- c("speaker","role","party","session_ref","session_indicator","speech")#
#
  ##############################
  #apply wordfish#
  ##############################
  text <- session_data$speech#
  labels <- data.frame(party=session_data$party,speaker=session_data$speaker,session_ref=session_data$session_ref,session_indicator=session_data$session_indicator)#
  if(length(unique(labels$party))>=2){#
  text.corpus   <- Corpus(VectorSource(text))#
#
  text.corpus <- tm_map(text.corpus, removeWords, custom_list) #
  # documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T,weighting = function(x) weightTfIdf(x, normalize = FALSE)))#
  documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T))#
  docs_f      <- removeSparseTerms(documents, 0.80)#
  wcdata      <- wfm(as.matrix(docs_f))#
  rowTotals   <- apply(wcdata , 1, sum) #Find the sum of words in each Document#
  wcdata      <- wcdata[rowTotals> 0, ]#
  wcdata <- wcdata[,colSums(wcdata)>0]#
#
  rownumbers <- 1:nrow(labels)#
  labels <- data.frame(number=rownumbers,party=labels$party,speaker=labels$speaker,session_indicator=labels$session_indicator,session_ref=labels$session_ref)#
  colnames(labels) <- c("number","party","speaker","session_indicator","session_ref")#
  dir1 <- 1#
  dir2 <- dim(wcdata)[2]#
  if(dim(wcdata)[2]>=5){#
  results     <- wordfish(wfm=wcdata,dir=c(dir1,dir2))#
  theta_file    <- data.frame(party_id = results$docs, theta=results$theta,se.theta=results$se.theta,party=labels$party[rownames(labels) %in% results$docs],speaker=labels$speaker[rownames(labels) %in% results$docs])#
  theta_file$session_ref <- rep(unique(labels$session_ref),nrow(theta_file))#
  theta_file$session_indicator <- rep(unique(labels$session_indicator),nrow(theta_file))#
  theta_file$theta <- scale(theta_file$theta,center=T,scale=T) # rescale with mean = center, variance = 1#
  ##############################
  #apply CA#
  ##############################
  ca_results  <- ca(wcdata)#
  ca_file  <- data.frame(score=ca_results$colcoord[,1])#
  ca_file$score <- scale(ca_file$score,center=T,scale=T) # rescale with mean = center, variance = 1 #
#
  s <- dim(t(wcdata))[2]#
#
   #check whether the first dimension accounts for more than what we would expect at random#
    random_accounts <- 100/(s-1)#
    singular_values <- ca_results$sv#
    inertia <- singular_values^2#
    pct <- 100*inertia/sum(inertia) #
    actual_account <- ca_results#
      x <- 0#
      for (p in pct){#
        if (p > random_accounts){#
          inc(x)#
        }}#
#
    dimensions <- x#
    perc.dim1 <- pct[1]#
#
    ca_file$dimensions <- rep(dimensions,nrow(ca_file))#
    ca_file$perc.dim1 <- rep(perc.dim1,nrow(ca_file))#
#
    ##############################
    #Combine CA and Wordfish results in one file#
    ##############################
    combined_results <- data.frame(party_id=theta_file$party_id,theta=theta_file$theta,se.theta=theta_file$se.theta,party=theta_file$party,speaker=theta_file$speaker,session_ref=theta_file$session_ref,session_indicator=theta_file$session_indicator,score=ca_file$score,dimensions=ca_file$dimensions,perc.dim1=ca_file$perc.dim1)#
    print(theta_file$session_ref[1])#
    #appends results to csv#
    setwd("/nfs/projects_nobackup/n/ngoet/dphil_machine_learning/matrices_no_procedural_strip")#
#
    if(s == 1){#
    write.table(combined_results, "full_scaling_estimates_nps.csv", sep = ",", col.names = T, append = T,row.names=F)}#
    else{write.table(combined_results, "full_scaling_estimates_nps.csv", sep = ",", col.names = F, append = T,row.names=F)}#
#
}#
rm(list=c("text.corpus","documents","docs_f","wcdata","theta_file","ca_file"))#
} #end loop over data_per session
nchar("The minister of")
nchar("The minister of transport")
5*8*4
20*8
3.5*16.72
seq(1811,2015,20)
seq(1811,2015,10)
seq(1820,2015,10)
x <- seq(1811,2015,1)
length(x)
x/10
205/10
205/11
205/7
205/8
205/10
205/12
205/15
205/14
205/9
205/10
205/5
x <- seq(1811,2015,5)
x
x <- seq(1811,2015,10)
x
x <- seq(1816,2015,5)
x
x <- seq(1811,2015,5)
x+5
x <- seq(1811,2015,10)
x
x <- seq(1811,2015,6)
x
x <- seq(1811,2015,12)
x
x+12
seq(1797,2015,1)
length(seq(1797,2015,1))
183+395+ 361+ 387+ 405 + 785 + 459 + 765 + 1000 + 878 + 873 + 903 + 904 + 1062 + 977 + 785 + 761 + 846 + 869 + 1345 + 707 + 371 + 580
5920/16601
length(seq(1703,1967,1))
.2*1931
library(plotly)#
#
data <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv")#
#
p <- plot_ly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) %>%#
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
library(plotly)#
#
data <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv")#
#
p <- plot_ly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) #
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
install.packages("magrittr")#
library(magrittr)
data <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv")#
#
p <- plot_ly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) %>%#
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
library(plotly)#
#
data <- read.csv("https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv")#
#
p <- plot_ly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) %>%#
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
p <- plotly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) %>%#
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
p <- plot_ly(data, x = ~., y = ~., text = ~School, type = 'scatter', mode = 'markers',#
        marker = list(size = ~gap, opacity = 0.5)) %>%#
  layout(title = 'Gender Gap in Earnings per University',#
         xaxis = list(showgrid = FALSE),#
         yaxis = list(showgrid = FALSE))
0.00039884+0.01291500
0.01015-0.00328999-0.00338999
##########################################################
#
## Economic voting in Germany: Multinomial logit model ###
#
##########################################################
## Johannes Karreth#
#
## ICPSR Summer Program 2016#
library(R2jags)#
# Download the data from https://dbk.gesis.org/DBKSearch/download.asp?id=37837#
d <- rio::import("2009_ZA5384_v1-0-0.dta")#
# Data cleaning and recoding#
d$votechoice <- NA#
#
d$votechoice <- ifelse(d$v3c == 1, 1, d$votechoice) # CDU/CSU#
#
d$votechoice <- ifelse(d$v3c == 2, 2, d$votechoice) # SPD#
#
d$votechoice <- ifelse(d$v3c == 3, 3, d$votechoice) # FDP#
#
d$votechoice <- ifelse(d$v3c == 4, 4, d$votechoice) # Linke#
#
d$votechoice <- ifelse(d$v3c == 5, 5, d$votechoice) # Green#
d$interest <- d$v15 - 3#
#
d$interest <- ifelse(d$interest == 3, NA, d$interest)#
d$leftright <- d$v56 - 6#
#
d$leftright <- ifelse(d$leftright == 6, NA, d$leftright)#
d$age <- d$vb - 5#
d$children <- ifelse(d$vx1 == 1, 1, ifelse(d$vx1 == 2, 0, NA))#
d$education <- ifelse(d$vf == 4 | d$vf == 5, -1, ifelse(d$vf == 1, 0, ifelse(d$vf == 2, 1, ifelse(d$vf == 3, 2, NA))))#
d$union <- ifelse(d$vp == 1 | d$vp == 2 | d$vp == 3, 1, ifelse(d$vp == 4, 0, NA))#
d$female <- d$va- 1#
d$econcountrynow <- ifelse(d$v25 == 1, 1, ifelse(d$v25 == 2, 0, ifelse(d$v25 == 3, 0, NA)))#
d$econselfnow <- ifelse(d$v27 == 1, 1, ifelse(d$v27 == 2, 0, ifelse(d$v27 == 3, 0, NA)))#
d$econctryworse <- ifelse(d$v25 == 1, 0, ifelse(d$v25 == 2, 0, ifelse(d$v25 == 3, 1, NA)))#
d$econselfworse <- ifelse(d$v27 == 1, 0, ifelse(d$v27 == 2, 0, ifelse(d$v27 == 3, 1, NA)))#
d$econcountryfuture <- ifelse(d$v29 == 2, -1, ifelse(d$v29 == 3, 0, ifelse(d$v29 == 1, 1, NA)))#
d$econselffuture <- ifelse(d$v28 == 3, -1, ifelse(d$v28 == 2, 0, ifelse(d$v29 == 1, 1, NA)))#
d$fearterror <- ifelse(d$v43 == 1, 1, ifelse(d$v43 == 2, 0, NA))#
d$class <- ifelse(d$vl2 == 1, -1, ifelse(d$vl2 == 2, 0, ifelse(d$vl2 == 3, 1, NA)))#
d <- d[, c("votechoice", "interest", "leftright", "age", "children", "education", "union", "female", "econctryworse", "econselfworse")]#
# Data for JAGS/BUGS#
d_nm <- na.omit(d[, c("votechoice", "interest", "leftright", "age", "children", "education", "union", "female", "econctryworse", "econselfworse")])#
#
dj <- as.list(d_nm)#
#
dj$N <- nrow(d_nm)#
#
dj$J <- length(as.numeric(levels(as.factor(dj$votechoice))))#
econ.mod <- function()  {#
  for(i in 1:N){#
#
    votechoice[i] ~ dcat(p[i, 1:J])#
    for (j in 1:J){#
#
      log(q[i,j]) <-  b[1,j] + #
#
        b[2,j] * interest[i] + #
#
        b[3,j] * leftright[i] + #
#
        b[4,j] * age[i] + #
#
        b[5,j] * children[i] + #
#
        b[6,j] * education[i] + #
#
        b[7,j] * union[i] + #
#
        b[8,j] * female[i] +#
#
        b[9,j] * econctryworse[i] + #
#
        b[10,j] * econselfworse[i]#
#
      s#
#
      p[i,j] <- q[i,j]/sum(q[i,1:J])  ## should be familiar from MLE notes: q is exp(Xb)#
#
    }   # close J loop#
#
  }  # close N loop#
  for(k in 1:10){#
#
    b[k,1] <- 0          ## MUST set the first set of covariates (for the first outcome category) to 0#
#
    for(j in 2:J){#
#
      b[k,j] ~ dnorm(0, 0.1)#
#
    }  # close J loop#
#
  }  # close K loop#
#
}  # close model loop #
econ.params <- c("b")#
econ.inits <- function(){#
#
  list(b = matrix(c(rep(NA, 10), #
#
                    rep(0, 40)), #
#
                  nrow = 10, ncol = 5, byrow = FALSE))#
#
}#
library(R2jags)#
#
econ.fit <- jags(data = dj, inits = econ.inits, econ.params, n.chains = 3, n.iter = 10, n.burnin = 5, n.thin = 1, model.file = econ.mod)#
#
econ.mcmc <- as.mcmc(econ.fit)#
#
library(mcmcplots)#
#
mcmcplot(econ.mcmc)#
econ.mcmc.dat <- as.data.frame(as.matrix(econ.mcmc))#
#
devtools::source_url("https://raw.githubusercontent.com/jkarreth/JKmisc/master/mcmctab.R")#
#
mcmctab(econ.mcmc)
library(mcmcplots)
?caterplot
86.48+ 35.00
168*260
getwd()
PATH <- dirname(sys.frame(1)$ofile)
scr_dir <- dirname(sys.frame(1)$ofile)
100000 - 77210 - 10000
12790 - 250
x <- read.csv("~/Desktop/ExtraAppendixWork/data_replication/Raw_speeches/speeches_Dail_29.tab",sep="\t")
x <- read.csv("~/Desktop/ExtraAppendixWork/data_replication/Raw_speeches/speeches_Dail_processed29.tab",sep="\t")
head(x)
tail(x)
CHdat <- read.csv("~/Desktop/NLworkPackage/CH_data/ELEN_CH.csv")
head(CHdat)
candVotes <- aggregate(elec_entry_id~candidate_votes,data=CHdat,FUN=sum)
CHdat <- read.csv("~/Desktop/NLworkPackage/CH_data/ELEN_CH.csv",stringsAsFactors=F)
head(CHdat)
candVotes <- aggregate(elec_entry_id~candidate_votes,data=CHdat,FUN=sum)
head(candVotes)
str(CHdat$candidate_votes)
str(CHdat)
head(candVotes)
candVotes <- aggregate(elec_entry_id~candidate_votes,data=CHdat,FUN=sum)
candVotes <- aggregate(as.factor(elec_entry_id)~candidate_votes,data=CHdat,FUN=sum)
candVotes <- aggregate(candidate_votes~ elec_entry_id,data=CHdat,FUN=sum)
head(candVotes)
write.csv(candVotes,"aggregatedCandVotes.csv")
pwd()
write.csv(candVotes,"~/Desktop/NLworkPackage/CH_data/aggregatedCandVotes.csv")
1000/3.208
(1000-95)/3.208
(1000+95)/3.208
(5000-90)/61.94
(5000-95)/9.69
(5000-95)/9.570
System(python3)
system(python3)
system('python3')
system('which python3')
x <- system('which python3')
x
print(system('which python3'))
system('python',wait=F)
system('which python',wait=F)
system('which python3',wait=F)
system('which python',wait=F)
x <- "machine_learning_implementations/1b_estimates_ps/1b_sgd_individual_estimates1123"
gsub("machine_learning_implementations/1b_estimates_ps/1b_sgd_individual_estimates",x)
gsub("machine_learning_implementations/1b_estimates_ps/1b_sgd_individual_estimates","",x)
x <- seq(1,10,1)
y <- seq(2,11,1)
data.frame(cor.test(x,y))
cor.test(x,y)
pl <- cor.test(x,y)
data.frame(pl)
pl$statistic
round(cor.test(x,y)$estimate,digits=2)
setwd('/Volumes/Back-ups/UKHCPOL/poisson_model_implementations')
library(data.table)
fread('debate_level_estimates_ps.csv')
x <- fread('debate_level_estimates_ps.csv')
head(x)
nrow(x)
x <- fread('debate_level_estimates_ps.csv')
x <- fread('debate_level_estimates_ps.csv')
head(x)
nrow(x)
x <- fread('debate_level_estimates_ps.csv')
x <- fread('debate_level_estimates_ps.csv')
x <- fread('debate_level_estimates_ps.csv')
head(x)
nrow(x)
x <- fread('debate_level_estimates_ps.csv')
x <- fread('debate_level_estimates_ps_process_22914.csv')
head(x)
tail(x)
x <- fread('debate_level_estimates_ps_process_22914.csv')
head(x)
nrow(x)
x <- fread('debate_level_estimates_ps_process_22914.csv')
x <- fread('debate_level_estimates_ps_process_22914.csv')
x
x <- fread('debate_level_estimates_ps_process_22914.csv')
nrow(x)
tail(x)
tail(x)
x <- fread('debate_level_estimates_ps_process_22914.csv')
nrow(x)
x <- fread('debate_level_estimates_ps_process_22914.csv')
nrow(x)
12/71
12/71*60
170/10
170(12/71*60)
170/(12/71*60)
x <- fread('debate_level_estimates_ps_process_22914.csv')
nrow(x)
x <- fread('debate_level_estimates_ps_process_22916.csv')
nrow(x)
x <- fread('debate_level_estimates_ps_process_22916.csv')
nrow(x)
x <- fread('debate_level_estimates_ps_process_22916.csv')
nrow(x)
x <- fread('debate_level_estimates_ps_process.csv')
x <- fread('debate_level_estimates_ps.csv')
head(x)
tail(x)
x <- subset(x, -V1)
x <- subset(x, select=-V1)
head(x)
write(x, 'debate_level_estimates_ps.csv',row.names=F)
write.csv(x, 'debate_level_estimates_ps.csv',row.names=F)
unique(x$session_ref)
