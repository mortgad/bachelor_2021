#Replication material for "Measuring Polarisation with Text Analysis: Evidence from the UK House of Commons, 1811-2015"
#=================================
set.seed(12345)
dataPath <- gsub("per_debate_scaling.R","",rstudioapi::getSourceEditorContext()$path)
setwd(dataPath)
library(base)
library(ca)
library(Hmisc)
library(data.table)
#create incrementer
inc <- function(x)
{
eval.parent(substitute(x <- x + 1))
}
custom_list <- c("a","an","and","are","as","at","be","by","for","from",
"has","he","in","is","it","its","of","on","that","the",
"to","was","were","will","with")
#load dataset
system.time(dataset <- fread("../../raw_data/ukhcdeb_ppr.csv",header=T))
fromSession <- 45.1
dataset <- dataset[dataset$session_ref >= fromSession,]
#install.packages("tm")
library(tm)
#devtools::install_github("conjugateprior/austin")
library(austin)
library(methods)
library(data.table)
library(stringr)
#create an indicator to split on
debate_indicator <- 0
#divide dataset by session
data_per_session <- split(dataset,dataset$session_ref)
#loop over
loop <- 1
for(i in 1:length(data_per_session)){
#store data for this loop
session_data <- data_per_session[[i]]
session_data$speech_content <- gsub('\"','',session_data$speech_content)
session_data <- session_data[nchar(session_data$matched_name)>0,]
session_data <- session_data[nchar(session_data$matched_name)<25,]
session_data$matched_name <- word(session_data$matched_name,-1)
#aggregate all speeches by member and party
#############################
#now enter the debate level
#############################
session_data <- transform(session_data,unique_id=as.numeric(interaction(date,year,debate_topic,session_ref)))
if(length(unique(session_data$unique_id)) >=2) {
debate_datasets <- split(session_data,session_data$unique_id)
for(db in 1:length(debate_datasets)){ #start debate_level loop
debate_data <- debate_datasets[[db]]
debate_data <- aggregate(debate_data$speech_content,by=list(debate_data$matched_name,debate_data$role,debate_data$party,debate_data$session_ref,debate_data$session_indicator,debate_data$year,debate_data$date,debate_data$debate_topic),paste,collapse=",")
colnames(debate_data) <- c("speaker","role","party","session_ref","session_indicator","year","date","title","speech")
#############################
#apply wordfish
#############################
text <- debate_data$speech
labels <- data.frame(party=debate_data$party,speaker=debate_data$matched_name,session_ref=debate_data$session_ref,session_indicator=debate_data$session_indicator,year = debate_data$year, date=debate_data$date, title=debate_data$title)
if(length(unique(labels$party))>=2){
#increment debate_indicator
inc(debate_indicator)
text.corpus   <- Corpus(VectorSource(text))
text.corpus <- tm_map(text.corpus, removeWords, custom_list)
# documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T,weighting = function(x) weightTfIdf(x, normalize = FALSE)))
documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T))
docs_f      <- removeSparseTerms(documents, 0.80)
wcdata      <- wfm(as.matrix(docs_f))
rowTotals   <- apply(wcdata , 1, sum) #Find the sum of words in each Document
wcdata      <- wcdata[rowTotals> 0, ]
wcdata <- wcdata[,colSums(wcdata)>0]
rownumbers <- 1:nrow(labels)
labels$number <- rownumbers
dir1 <- 1
dir2 <- dim(wcdata)[2]
if(dim(wcdata)[2]>=5){
results     <- wordfish(wfm=wcdata,dir=c(dir1,dir2))
theta_file    <- data.frame(id = results$docs, theta=results$theta,se.theta=results$se.theta,party=labels$party[rownames(labels) %in% results$docs],speaker=labels$matched_name[rownames(labels) %in% results$docs],year=labels$year[rownames(labels) %in% results$docs],date=labels$date[rownames(labels) %in% results$docs],title=labels$title[rownames(labels) %in% results$docs])
theta_file$session_ref <- rep(unique(labels$session_ref),nrow(theta_file))
theta_file$session_indicator <- rep(unique(labels$session_indicator),nrow(theta_file))
theta_file$debate_indicator <- rep(debate_indicator,nrow(theta_file))
theta_file$theta <- scale(theta_file$theta,center=T,scale=T) # rescale with mean = center, variance = 1
#############################
#apply CA
#############################
ca_results  <- ca(wcdata)
ca_file  <- data.frame(score=ca_results$colcoord[,1])
ca_file$score <- scale(ca_file$score,center=T,scale=T) # rescale with mean = center, variance = 1
s <- dim(t(wcdata))[2]
#check whether the first dimension accounts for more than what we would expect at random
random_accounts <- 100/(s-1)
singular_values <- ca_results$sv
inertia <- singular_values^2
pct <- 100*inertia/sum(inertia)
actual_account <- ca_results
x <- 0
for (p in pct){
if (p > random_accounts){
inc(x)
}}
dimensions <- x
perc.dim1 <- pct[1]
ca_file$dimensions <- rep(dimensions,nrow(ca_file))
ca_file$perc.dim1 <- rep(perc.dim1,nrow(ca_file))
#############################
#Combine CA and Wordfish results in one file
#############################
combined_results <- data.frame(id=theta_file$id,theta=theta_file$theta,se.theta=theta_file$se.theta,party=theta_file$party,speaker=theta_file$matched_name,session_ref=theta_file$session_ref,session_indicator=theta_file$session_indicator,year=theta_file$year,date=theta_file$date,title=theta_file$title,debate_indicator = theta_file$debate_indicator,score=ca_file$score,dimensions=ca_file$dimensions,perc.dim1=ca_file$perc.dim1)
#appends results to csv
setwd(dataPath)
if(loop == 1){
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = T, append = T,row.names=F)}else{
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = F, append = T,row.names=F)
}
}
}
} #end loop over data_per session
} #end debate-level loop
print(paste("Writing out results for session ",combined_results$session_ref[1],sep=""))
} #end year-level loop
#=================================
#First version: 5th September 2017
#This version: 5th June 2018
#Author: Niels Goet
#Replication material for "Measuring Polarisation with Text Analysis: Evidence from the UK House of Commons, 1811-2015"
#=================================
set.seed(12345)
dataPath <- gsub("per_debate_scaling.R","",rstudioapi::getSourceEditorContext()$path)
setwd(dataPath)
library(base)
library(ca)
library(Hmisc)
library(data.table)
library(stringr)
#create incrementer
inc <- function(x)
{
eval.parent(substitute(x <- x + 1))
}
custom_list <- c("a","an","and","are","as","at","be","by","for","from",
"has","he","in","is","it","its","of","on","that","the",
"to","was","were","will","with")
#load dataset
system.time(dataset <- fread("../../raw_data/ukhcdeb_ppr.csv",header=T))
fromSession <- 45.1
dataset <- dataset[dataset$session_ref >= fromSession,]
#install.packages("tm")
library(tm)
#devtools::install_github("conjugateprior/austin")
library(austin)
library(methods)
library(data.table())
#create an indicator to split on
debate_indicator <- 0
#divide dataset by session
data_per_session <- split(dataset,dataset$session_ref)
#loop over
loop <- 1
for(i in 1:length(data_per_session)){
#store data for this loop
session_data <- data_per_session[[i]]
session_data$speech_content <- gsub('\"','',session_data$speech_content)
#aggregate all speeches by member and party
#############################
#now enter the debate level
#############################
session_data <- transform(session_data,unique_id=as.numeric(interaction(date,year,debate_topic,session_ref)))
if(length(unique(session_data$unique_id)) >=2) {
debate_datasets <- split(session_data,session_data$unique_id)
for(db in 1:length(debate_datasets)){ #start debate_level loop
debate_data <- debate_datasets[[db]]
if(nrow(debate_data) >= 5){
temp_names <- strsplit(debate_data$matched_name," ")
names <- list()
for(n in 1:length(temp_names)){
temp_name <- temp_names[[n]]
if(length(temp_name)>1){
name <- c(substr(temp_name[1],1,1),unlist(temp_name[seq(2,length(temp_name),1)]))
name <- paste(name,collapse=" ")}
else{name <- temp_name}
names[[n]] <- name
}
library(plyr)
names <- unlist(names)
debate_data$matched_name <- names
debate_data <- aggregate(debate_data$speech_content,by=list(debate_data$matched_name,debate_data$role,debate_data$party,debate_data$session_ref,debate_data$session_indicator,debate_data$year,debate_data$date,debate_data$debate_topic),paste,collapse=",")
colnames(debate_data) <- c("speaker","role","party","session_ref","session_indicator","year","date","title","speech")
#############################
#apply wordfish
#############################
text <- debate_data$speech
labels <- data.frame(party=debate_data$party,speaker=debate_data$speaker,session_ref=debate_data$session_ref,session_indicator=debate_data$session_indicator,year = debate_data$year, date=debate_data$date, title=debate_data$title)
if(length(unique(labels$party))>=2){
#increment debate_indicator
inc(debate_indicator)
text.corpus   <- Corpus(VectorSource(text))
text.corpus <- tm_map(text.corpus, removeWords, custom_list)
# documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T,weighting = function(x) weightTfIdf(x, normalize = FALSE)))
documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T))
docs_f      <- removeSparseTerms(documents, 0.80)
wcdata      <- wfm(as.matrix(docs_f))
rowTotals   <- apply(wcdata , 1, sum) #Find the sum of words in each Document
wcdata      <- wcdata[rowTotals> 0, ]
wcdata <- wcdata[,colSums(wcdata)>0]
rownumbers <- 1:nrow(labels)
labels$number <- rownumbers
dir1 <- 1
dir2 <- dim(wcdata)[2]
if(dim(wcdata)[2]>=5){
results     <- wordfish(wfm=wcdata,dir=c(dir1,dir2))
theta_file    <- data.frame(id = results$docs, theta=results$theta,se.theta=results$se.theta,party=labels$party[rownames(labels) %in% results$docs],speaker=labels$speaker[rownames(labels) %in% results$docs],year=labels$year[rownames(labels) %in% results$docs],date=labels$date[rownames(labels) %in% results$docs],title=labels$title[rownames(labels) %in% results$docs])
theta_file$session_ref <- rep(unique(labels$session_ref),nrow(theta_file))
theta_file$session_indicator <- rep(unique(labels$session_indicator),nrow(theta_file))
theta_file$debate_indicator <- rep(debate_indicator,nrow(theta_file))
theta_file$theta <- scale(theta_file$theta,center=T,scale=T) # rescale with mean = center, variance = 1
#############################
#apply CA
#############################
ca_results  <- ca(wcdata)
ca_file  <- data.frame(score=ca_results$colcoord[,1])
ca_file$score <- scale(ca_file$score,center=T,scale=T) # rescale with mean = center, variance = 1
s <- dim(t(wcdata))[2]
#check whether the first dimension accounts for more than what we would expect at random
random_accounts <- 100/(s-1)
singular_values <- ca_results$sv
inertia <- singular_values^2
pct <- 100*inertia/sum(inertia)
actual_account <- ca_results
x <- 0
for (p in pct){
if (p > random_accounts){
inc(x)
}}
dimensions <- x
perc.dim1 <- pct[1]
ca_file$dimensions <- rep(dimensions,nrow(ca_file))
ca_file$perc.dim1 <- rep(perc.dim1,nrow(ca_file))
#############################
#Combine CA and Wordfish results in one file
#############################
combined_results <- data.frame(id=theta_file$id,theta=theta_file$theta,se.theta=theta_file$se.theta,party=theta_file$party,speaker=theta_file$speaker,session_ref=theta_file$session_ref,session_indicator=theta_file$session_indicator,year=theta_file$year,date=theta_file$date,title=theta_file$title,debate_indicator = theta_file$debate_indicator,score=ca_file$score,dimensions=ca_file$dimensions,perc.dim1=ca_file$perc.dim1)
#appends results to csv
setwd(dataPath)
if(loop == 1){
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = T, append = T,row.names=F)}else{
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = F, append = T,row.names=F)
}
}
}
} #end loop over data_per session
} #end debate-level loop
}
print(paste("Writing out results for session ",combined_results$session_ref[1],sep=""))
} #end year-level loop
#=================================
#First version: 5th September 2017
#This version: 5th June 2018
#Author: Niels Goet
#Replication material for "Measuring Polarisation with Text Analysis: Evidence from the UK House of Commons, 1811-2015"
#=================================
set.seed(12345)
dataPath <- gsub("per_debate_scaling.R","",rstudioapi::getSourceEditorContext()$path)
setwd(dataPath)
library(base)
library(ca)
library(Hmisc)
library(data.table)
library(stringr)
#create incrementer
inc <- function(x)
{
eval.parent(substitute(x <- x + 1))
}
custom_list <- c("a","an","and","are","as","at","be","by","for","from",
"has","he","in","is","it","its","of","on","that","the",
"to","was","were","will","with")
#load dataset
system.time(dataset <- fread("../../raw_data/ukhcdeb_ppr.csv",header=T))
fromSession <- 45.1
dataset <- dataset[dataset$session_ref >= fromSession,]
#install.packages("tm")
library(tm)
#devtools::install_github("conjugateprior/austin")
library(austin)
library(methods)
library(data.table())
#create an indicator to split on
debate_indicator <- 0
#divide dataset by session
data_per_session <- split(dataset,dataset$session_ref)
#loop over
loop <- 2
for(i in 1:length(data_per_session)){
#store data for this loop
session_data <- data_per_session[[i]]
session_data$speech_content <- gsub('\"','',session_data$speech_content)
#aggregate all speeches by member and party
#############################
#now enter the debate level
#############################
session_data <- transform(session_data,unique_id=as.numeric(interaction(date,year,debate_topic,session_ref)))
if(length(unique(session_data$unique_id)) >=2) {
debate_datasets <- split(session_data,session_data$unique_id)
for(db in 1:length(debate_datasets)){ #start debate_level loop
debate_data <- debate_datasets[[db]]
if(nrow(debate_data) >= 5){
temp_names <- strsplit(debate_data$matched_name," ")
names <- list()
for(n in 1:length(temp_names)){
temp_name <- temp_names[[n]]
if(length(temp_name)>1){
name <- c(substr(temp_name[1],1,1),unlist(temp_name[seq(2,length(temp_name),1)]))
name <- paste(name,collapse=" ")}
else{name <- temp_name}
names[[n]] <- name
}
library(plyr)
names <- unlist(names)
debate_data$matched_name <- names
debate_data <- aggregate(debate_data$speech_content,by=list(debate_data$matched_name,debate_data$role,debate_data$party,debate_data$session_ref,debate_data$session_indicator,debate_data$year,debate_data$date,debate_data$debate_topic),paste,collapse=",")
colnames(debate_data) <- c("speaker","role","party","session_ref","session_indicator","year","date","title","speech")
#############################
#apply wordfish
#############################
text <- debate_data$speech
labels <- data.frame(party=debate_data$party,speaker=debate_data$speaker,session_ref=debate_data$session_ref,session_indicator=debate_data$session_indicator,year = debate_data$year, date=debate_data$date, title=debate_data$title)
if(length(unique(labels$party))>=2){
#increment debate_indicator
inc(debate_indicator)
text.corpus   <- Corpus(VectorSource(text))
text.corpus <- tm_map(text.corpus, removeWords, custom_list)
# documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T,weighting = function(x) weightTfIdf(x, normalize = FALSE)))
documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T))
docs_f      <- removeSparseTerms(documents, 0.80)
wcdata      <- wfm(as.matrix(docs_f))
rowTotals   <- apply(wcdata , 1, sum) #Find the sum of words in each Document
wcdata      <- wcdata[rowTotals> 0, ]
wcdata <- wcdata[,colSums(wcdata)>0]
rownumbers <- 1:nrow(labels)
labels$number <- rownumbers
dir1 <- 1
dir2 <- dim(wcdata)[2]
if(dim(wcdata)[2]>=5){
results     <- wordfish(wfm=wcdata,dir=c(dir1,dir2))
theta_file    <- data.frame(id = results$docs, theta=results$theta,se.theta=results$se.theta,party=labels$party[rownames(labels) %in% results$docs],speaker=labels$speaker[rownames(labels) %in% results$docs],year=labels$year[rownames(labels) %in% results$docs],date=labels$date[rownames(labels) %in% results$docs],title=labels$title[rownames(labels) %in% results$docs])
theta_file$session_ref <- rep(unique(labels$session_ref),nrow(theta_file))
theta_file$session_indicator <- rep(unique(labels$session_indicator),nrow(theta_file))
theta_file$debate_indicator <- rep(debate_indicator,nrow(theta_file))
theta_file$theta <- scale(theta_file$theta,center=T,scale=T) # rescale with mean = center, variance = 1
#############################
#apply CA
#############################
ca_results  <- ca(wcdata)
ca_file  <- data.frame(score=ca_results$colcoord[,1])
ca_file$score <- scale(ca_file$score,center=T,scale=T) # rescale with mean = center, variance = 1
s <- dim(t(wcdata))[2]
#check whether the first dimension accounts for more than what we would expect at random
random_accounts <- 100/(s-1)
singular_values <- ca_results$sv
inertia <- singular_values^2
pct <- 100*inertia/sum(inertia)
actual_account <- ca_results
x <- 0
for (p in pct){
if (p > random_accounts){
inc(x)
}}
dimensions <- x
perc.dim1 <- pct[1]
ca_file$dimensions <- rep(dimensions,nrow(ca_file))
ca_file$perc.dim1 <- rep(perc.dim1,nrow(ca_file))
#############################
#Combine CA and Wordfish results in one file
#############################
combined_results <- data.frame(id=theta_file$id,theta=theta_file$theta,se.theta=theta_file$se.theta,party=theta_file$party,speaker=theta_file$speaker,session_ref=theta_file$session_ref,session_indicator=theta_file$session_indicator,year=theta_file$year,date=theta_file$date,title=theta_file$title,debate_indicator = theta_file$debate_indicator,score=ca_file$score,dimensions=ca_file$dimensions,perc.dim1=ca_file$perc.dim1)
#appends results to csv
setwd(dataPath)
if(loop == 1){
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = T, append = T,row.names=F)}else{
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = F, append = T,row.names=F)
}
}
}
} #end loop over data_per session
} #end debate-level loop
}
print(paste("Writing out results for session ",combined_results$session_ref[1],sep=""))
} #end year-level loop
warnings()
matched_name
#=================================
#First version: 5th September 2017
#This version: 5th June 2018
#Author: Niels Goet
#Replication material for "Measuring Polarisation with Text Analysis: Evidence from the UK House of Commons, 1811-2015"
#=================================
set.seed(12345)
dataPath <- gsub("per_debate_scaling.R","",rstudioapi::getSourceEditorContext()$path)
setwd(dataPath)
library(base)
library(ca)
library(Hmisc)
library(data.table)
library(stringr)
#create incrementer
inc <- function(x)
{
eval.parent(substitute(x <- x + 1))
}
custom_list <- c("a","an","and","are","as","at","be","by","for","from",
"has","he","in","is","it","its","of","on","that","the",
"to","was","were","will","with")
#load dataset
system.time(dataset <- fread("../../raw_data/ukhcdeb_ppr.csv",header=T))
fromSession <- 47.1
dataset <- dataset[dataset$session_ref >= fromSession,]
#install.packages("tm")
library(tm)
#devtools::install_github("conjugateprior/austin")
library(austin)
library(methods)
library(data.table())
#create an indicator to split on
debate_indicator <- 0
#divide dataset by session
data_per_session <- split(dataset,dataset$session_ref)
#loop over
loop <- 1
for(i in 1:length(data_per_session)){
#store data for this loop
session_data <- data_per_session[[i]]
session_data$speech_content <- gsub('\"','',session_data$speech_content)
session_data <- session_data[nchar(session_data$matched_name)>0,]
session_data <- session_data[nchar(session_data$matched_name)<25,]
session_data$matched_name <- word(session_data$matched_name,-1)
#aggregate all speeches by member and party
#############################
#now enter the debate level
#############################
session_data <- transform(session_data,unique_id=as.numeric(interaction(date,year,debate_topic,session_ref)))
if(length(unique(session_data$unique_id)) >=2) {
debate_datasets <- split(session_data,session_data$unique_id)
for(db in 1:length(debate_datasets)){ #start debate_level loop
debate_data <- debate_datasets[[db]]
debate_data <- aggregate(debate_data$speech_content,by=list(debate_data$matched_name,debate_data$role,debate_data$party,debate_data$session_ref,debate_data$session_indicator,debate_data$year,debate_data$date,debate_data$debate_topic),paste,collapse=",")
colnames(debate_data) <- c("matched_name","role","party","session_ref","session_indicator","year","date","title","speech")
#############################
#apply wordfish
#############################
text <- debate_data$speech
labels <- data.frame(party=debate_data$party,speaker=debate_data$matched_name,session_ref=debate_data$session_ref,session_indicator=debate_data$session_indicator,year = debate_data$year, date=debate_data$date, title=debate_data$title)
if(length(unique(labels$party))>=2){
#increment debate_indicator
inc(debate_indicator)
text.corpus   <- Corpus(VectorSource(text))
text.corpus <- tm_map(text.corpus, removeWords, custom_list)
# documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T,weighting = function(x) weightTfIdf(x, normalize = FALSE)))
documents   <- TermDocumentMatrix(text.corpus, control = list(removePunctuation=T,tolower=T,removeNumbers=T))
docs_f      <- removeSparseTerms(documents, 0.80)
wcdata      <- wfm(as.matrix(docs_f))
rowTotals   <- apply(wcdata , 1, sum) #Find the sum of words in each Document
wcdata      <- wcdata[rowTotals> 0, ]
wcdata <- wcdata[,colSums(wcdata)>0]
rownumbers <- 1:nrow(labels)
labels$number <- rownumbers
dir1 <- 1
dir2 <- dim(wcdata)[2]
if(dim(wcdata)[2]>=5){
results     <- wordfish(wfm=wcdata,dir=c(dir1,dir2))
theta_file    <- data.frame(id = results$docs, theta=results$theta,se.theta=results$se.theta,party=labels$party[rownames(labels) %in% results$docs],speaker=labels$speaker[rownames(labels) %in% results$docs],year=labels$year[rownames(labels) %in% results$docs],date=labels$date[rownames(labels) %in% results$docs],title=labels$title[rownames(labels) %in% results$docs])
theta_file$session_ref <- rep(unique(labels$session_ref),nrow(theta_file))
theta_file$session_indicator <- rep(unique(labels$session_indicator),nrow(theta_file))
theta_file$debate_indicator <- rep(debate_indicator,nrow(theta_file))
theta_file$theta <- scale(theta_file$theta,center=T,scale=T) # rescale with mean = center, variance = 1
#############################
#apply CA
#############################
ca_results  <- ca(wcdata)
ca_file  <- data.frame(score=ca_results$colcoord[,1])
ca_file$score <- scale(ca_file$score,center=T,scale=T) # rescale with mean = center, variance = 1
s <- dim(t(wcdata))[2]
#check whether the first dimension accounts for more than what we would expect at random
random_accounts <- 100/(s-1)
singular_values <- ca_results$sv
inertia <- singular_values^2
pct <- 100*inertia/sum(inertia)
actual_account <- ca_results
x <- 0
for (p in pct){
if (p > random_accounts){
inc(x)
}}
dimensions <- x
perc.dim1 <- pct[1]
ca_file$dimensions <- rep(dimensions,nrow(ca_file))
ca_file$perc.dim1 <- rep(perc.dim1,nrow(ca_file))
#############################
#Combine CA and Wordfish results in one file
#############################
combined_results <- data.frame(id=theta_file$id,theta=theta_file$theta,se.theta=theta_file$se.theta,party=theta_file$party,speaker=theta_file$speaker,session_ref=theta_file$session_ref,session_indicator=theta_file$session_indicator,year=theta_file$year,date=theta_file$date,title=theta_file$title,debate_indicator = theta_file$debate_indicator,score=ca_file$score,dimensions=ca_file$dimensions,perc.dim1=ca_file$perc.dim1)
#appends results to csv
setwd(dataPath)
if(loop == 1){
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = T, append = T,row.names=F)}else{
write.table(combined_results, "debate_level_estimates_psN.csv", sep = ",", col.names = F, append = T,row.names=F)
}
loop <- loop+1
}
}
} #end loop over data_per session
} #end debate-level loop
print(paste("Writing out results for session ",combined_results$session_ref[1],sep=""))
}
#end year-level loop
